{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e390247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc0833",
   "metadata": {},
   "source": [
    "## Reading Files, getting sentence tokens and converting them to word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2311eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = glob.glob(\"CUAD_v1/full_contract_txt/*.txt\")\n",
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148ea8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFiles(all_files):\n",
    "    all_tokens = []\n",
    "    for file in all_files:\n",
    "        with open(file, encoding='utf-8') as info:\n",
    "            sent_tokens = sent_tokenize(info.read())\n",
    "            for sent_token in sent_tokens:\n",
    "                all_tokens.extend(getTokens(sent_token))\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b9082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(sent_token):\n",
    "    tokens = []\n",
    "    for token in word_tokenize(sent_token):\n",
    "        tokens.append(token.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0aaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = readFiles(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f16e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4789850"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)\n",
    "\n",
    "# of tokens - 4789850 (first converted to lower case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c3f20",
   "metadata": {},
   "source": [
    "## Writing all tokens to output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d633e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = all_tokens\n",
    "temp = '\\n'.join(temp)\n",
    "# len(temp)\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "        f.write(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00161ff",
   "metadata": {},
   "source": [
    "## Counting all unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9783fb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens = Counter(all_tokens)\n",
    "len(count_tokens)\n",
    "\n",
    "# types of tokens - 45883"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30257395",
   "metadata": {},
   "source": [
    "## Creating a list of unique tokens, sorted by decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac84943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 257132),\n",
       " (',', 240576),\n",
       " ('of', 156122),\n",
       " ('to', 129875),\n",
       " ('and', 129054),\n",
       " ('.', 117513),\n",
       " ('or', 105155),\n",
       " ('in', 79933),\n",
       " (')', 78092),\n",
       " ('(', 75436),\n",
       " ('*', 67765),\n",
       " ('any', 62236),\n",
       " ('--', 58711),\n",
       " ('a', 51002),\n",
       " ('shall', 48794),\n",
       " ('by', 44311),\n",
       " ('agreement', 43622),\n",
       " ('this', 39987),\n",
       " ('be', 39701),\n",
       " ('for', 38724)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_tokens(d, reverse = False):\n",
    "  return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))\n",
    "\n",
    "sorted_tokens = sort_tokens(count_tokens, True)\n",
    "\n",
    "sorted_tokens_list = list(sorted_tokens.items())\n",
    "sorted_tokens_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ffe93",
   "metadata": {},
   "source": [
    "## Writing all unique tokens and their frequencies to tokens.txt, sorted by decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31753445",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ''\n",
    "for token_freq in sorted_tokens_list:\n",
    "    token_freq = str(token_freq)\n",
    "    temp = temp + token_freq + \"\\n\"\n",
    "    \n",
    "with open('tokens.txt', 'w') as f:\n",
    "        f.write(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f235b78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45883"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens - 45883\n",
    "\n",
    "unique_tokens = len(sorted_tokens)\n",
    "unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffbd8e",
   "metadata": {},
   "source": [
    "## Calculating type-to-token ratio for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2abaef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009579214380408572"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type/token ratio - 0.009579214380408572\n",
    "\n",
    "type_token_ratio = unique_tokens/len(all_tokens)\n",
    "\n",
    "type_token_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48268aa4",
   "metadata": {},
   "source": [
    "## Getting Count of unique tokens, that appeared only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262ebd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19649"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens appeared only once - 19649\n",
    "\n",
    "tokens_only_once = [k for k, v in sorted_tokens.items() if v == 1]\n",
    "\n",
    "len(tokens_only_once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b96bbf",
   "metadata": {},
   "source": [
    "## Extracting only words from list of tokens, removing punctuations, symbols etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335f6a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3841868"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_tokens = []\n",
    "\n",
    "for token in all_tokens:\n",
    "    token = token.strip(punctuation)\n",
    "    if token.isalpha():\n",
    "        all_word_tokens.append(token)\n",
    "        \n",
    "len(all_word_tokens)\n",
    "\n",
    "# number of words - 3841868"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42194db",
   "metadata": {},
   "source": [
    "## Lexical Diversity - type-to-token ratio after removing punctuations, symbols etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886ebb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006610065728442518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(all_word_tokens)) / len(all_word_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54205e7",
   "metadata": {},
   "source": [
    "## Getting Top 20 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fca19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(d, reverse = False):\n",
    "  return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7e69afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the | 257141 \n",
      "of | 156123 \n",
      "to | 129875 \n",
      "and | 129072 \n",
      "or | 105168 \n",
      "in | 79944 \n",
      "any | 62236 \n",
      "a | 51447 \n",
      "shall | 48794 \n",
      "by | 44311 \n",
      "agreement | 43638 \n",
      "this | 39989 \n",
      "be | 39701 \n",
      "for | 38724 \n",
      "such | 36173 \n",
      "with | 33884 \n",
      "as | 32910 \n",
      "party | 32831 \n",
      "that | 27654 \n",
      "other | 26395 \n"
     ]
    }
   ],
   "source": [
    "def getTopNMostFrequent(tokens_list, n):\n",
    "    count_word_tokens = Counter(tokens_list)\n",
    "    sorted_word_tokens = sort_dict_by_value(count_word_tokens, True)\n",
    "    for word_token in list(sorted_word_tokens)[0:n]:\n",
    "        print (\"{} | {} \".format(word_token, sorted_word_tokens[word_token]))\n",
    "    return sorted_word_tokens\n",
    "\n",
    "sorted_word_tokens = getTopNMostFrequent(all_word_tokens, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3936a",
   "metadata": {},
   "source": [
    "## Calculating Lexical Diversity - type-to-token ratio for words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea33961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006610065728442518"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexical Diversity - type-to-token ratio for words only - 0.00661\n",
    "\n",
    "unique_words_count = len(sorted_word_tokens)\n",
    "\n",
    "type_token_ratio_words = unique_words_count/len(all_word_tokens)\n",
    "type_token_ratio_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffef8f",
   "metadata": {},
   "source": [
    "## Excluding stopwords from list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea65cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(all_word_tokens):\n",
    "    with open('StopWords.txt', encoding='utf-8') as info:\n",
    "        stop_words_file = word_tokenize(info.read())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(stop_words_file)\n",
    "\n",
    "    words_without_stopwords = [w for w in all_word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    return words_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0407eb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3841868, 1818953)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_without_stopwords = removeStopWords(all_word_tokens)\n",
    "    \n",
    "len(all_word_tokens), len(words_without_stopwords)\n",
    "\n",
    "# (3841868, 1818953) - with stopwords from both file and nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df357b1",
   "metadata": {},
   "source": [
    "## Getting top 20 most frequent words after removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae333c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement | 43638 \n",
      "party | 32831 \n",
      "parties | 13511 \n",
      "section | 13292 \n",
      "company | 12403 \n",
      "information | 10923 \n",
      "product | 10766 \n",
      "date | 10127 \n",
      "products | 8169 \n",
      "rights | 8049 \n",
      "services | 7866 \n",
      "applicable | 7533 \n",
      "business | 7255 \n",
      "set | 6984 \n",
      "confidential | 6881 \n",
      "written | 6799 \n",
      "terms | 6714 \n",
      "right | 6676 \n",
      "notice | 6655 \n",
      "term | 6575 \n"
     ]
    }
   ],
   "source": [
    "top20_frequent_words = getTopNMostFrequent(words_without_stopwords, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5faba9",
   "metadata": {},
   "source": [
    "## Calculating Lexical density - type-to-token ratio when using only word tokens without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e033b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013619923109613057"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexical density - 0.0136199\n",
    "\n",
    "count_words_without_stopwords = Counter(words_without_stopwords)\n",
    "\n",
    "type_words_count = len(count_words_without_stopwords)\n",
    "words_count = len(words_without_stopwords) \n",
    "\n",
    "lexical_density = type_words_count/words_count\n",
    "\n",
    "lexical_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a0d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all bigrams from text after removing stopwords and punctuations\n",
    "\n",
    "def getBiGrams(all_files):\n",
    "    all_tokens = readFiles(all_files)\n",
    "    filtered_tokens = [token for token in all_tokens if token.isalpha()]\n",
    "    filtered_tokens = removeStopWords(filtered_tokens)\n",
    "    return nltk.bigrams(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3456db42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('confidential', 'information') | 3604 \n",
      "('intellectual', 'property') | 2921 \n",
      "('effective', 'date') | 2839 \n",
      "('written', 'notice') | 2387 \n",
      "('terms', 'conditions') | 2087 \n",
      "('set', 'section') | 1825 \n",
      "('prior', 'written') | 1814 \n",
      "('term', 'agreement') | 1709 \n",
      "('confidential', 'treatment') | 1534 \n",
      "('termination', 'agreement') | 1440 \n",
      "('parties', 'agree') | 1417 \n",
      "('securities', 'exchange') | 1410 \n",
      "('receiving', 'party') | 1367 \n",
      "('party', 'party') | 1363 \n",
      "('pursuant', 'section') | 1353 \n",
      "('written', 'consent') | 1330 \n",
      "('united', 'states') | 1265 \n",
      "('applicable', 'law') | 1249 \n",
      "('agreement', 'party') | 1215 \n",
      "('terms', 'agreement') | 1202 \n"
     ]
    }
   ],
   "source": [
    "biGrams = list(getBiGrams(all_files))\n",
    "top20_frequent_bigrams = getTopNMostFrequent(biGrams, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
